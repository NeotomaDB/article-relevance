{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Pipelines with `article_relevance`\n",
    "\n",
    "The Article relevance tool was designed for use with the [Neotoma Paleoecology Database](https://www.neotomadb.org) to produce a workflow that allows users to supply an updatable list of DOIs from publications. This list of DOIs is then used to extract metadata from [CrossRef](https://crossref.org) that can be constructed into a list of text embeddings from which we can develop predictive models.  The tooling allows us to generate multiple predictive models, along with the ability to perform a grid-search for optimal hyper-parameter tuning for all models.\n",
    "\n",
    "The workflow provides predictive outputs as to whether an article might be suited for inclusion into a research database, a probability estimate for the prediction, as well as time-stamped predictions and the ability of a user to override the prediction. In this way we can test model evolution, and provide the opportunity for curated stewardship of model predictions.\n",
    "\n",
    "![./assets/overview_image.svg](./assets/overview_image.svg)\n",
    "\n",
    "Our goal is to provide a fully developed research infrastructure that connects labelled publication data from a particular research database, to machine learning models trained with classification models to predict article relevance, the suitability of an \"unknown\" journal article for inclusion within the database.\n",
    "\n",
    "## Using the NeotomaART (Article Relevance Tool) Docker Container\n",
    "\n",
    "The [Docker Container](https://github.com/NeotomaDB/article_project) connects three separate elements:\n",
    "\n",
    "* A [Postgres database](https://github.com/NeotomaDB/article_project/tree/main/article_database) with a [pre-defined data schema](https://github.com/NeotomaDB/article_project/blob/main/article_database/create_database.sql).\n",
    "* A [node/Express API](https://github.com/NeotomaDB/article_project/tree/main/article_api) to interface with the database.\n",
    "* A Python package/framework to interact with the API.\n",
    "\n",
    "This structure allows a research team to set up their own local or cloud-based instance of the ART to support data discovery and ingest for particular research groups. The goal of this project is to allow research teams to submit publications relevant to their research project, along with other publications, all identified with DOIs. Models are then built using a range of parameters, saved, and can then be used to predict whether or not \"unseen\" articles are then relevant for the research group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the script\n",
    "\n",
    "Assuming that the Docker container is running, following the instructions within the [`README`](https://github.com/NeotomaDB/article_project/blob/main/README.md), we can load in the packages and begin to run the code. Note that a `.env` file is used here to help manage any environment variables we might need. At present there is only a single variable in `.env`: `API_HOME=\"localhost:8080\"`. This value is pulled from the [`docker-compose` file in `article_project`](https://github.com/NeotomaDB/article_project/blob/main/docker-compose.yml#L20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import article_relevance as ar\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `article_relevance` package includes multiple requirements for proper use. All requirements are placed in an `requirements.txt` file.\n",
    "\n",
    "In addition, at minimum, we require a file with properly labelled DOI data. This data should contain the following columns:\n",
    "\n",
    "* doi: A properly formatted DOI, with only the shoulder and endpoint. e.g., `10.5467/22343.whatever`\n",
    "* label: A categorical label that can be used to identify whether or not the article is suitable for inclusion into the database.\n",
    "\n",
    "It is also possible to load in unlabelled data as a list of DOI values. Throughout we use DOIs and ORCIDs as unique identifiers to link labels, articles, and predictions.\n",
    "\n",
    "This project contains two data resources in the `data` folder:\n",
    "\n",
    "* `raw/neotoma_crossref.csv`: data directly exported from Neotoma (publication is already included in Neotoma).\n",
    "* `raw/labelled_data.csv`: manually labelled data for model training.\n",
    "* `raw/project_2_labelled_data.csv`: data labelled using the [`SMART` application](https://github.com/RTIInternational/SMART)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These objects represent the file elements we'll be working with for the predictive models.\n",
    "\n",
    "## Loading Raw Data\n",
    "\n",
    "For our purposes we want to identify both the data source and the labelling. For any object we want to know the data source and certainty with which it was labelled. Data from our source should be most trustworthy, labelled data should have some indication of the labeller, unlabelled data should reflect that fact.\n",
    "\n",
    "The first set of data we load is raw data from the database itself (`db_data`), and a set of labelled data (`labelled_data.csv`). The labelled data was all labelled by the same person in this case (\"Simon Goring\"), so when we insert the data to the `LABELLING_STORE` document we'll make sure to add that.\n",
    "\n",
    "DOIs get entered in many ways by users. Incorrect data entry may result in DOIs that do not properly resolve, or are simply incorrect. Additionaly data entry errors may result in leading or trailing whitespace. To help us ensure that data is entered correctly we will use the function `ar.clean_dois()`. This function returns a `dict` with the `clean` and `removed` DOIs submitted by the user. This helps support data cleaning down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2652 good DOIs and 48 removed DOIs in the full set of DOIs submitted.\n"
     ]
    }
   ],
   "source": [
    "with open('data/raw/neotoma_dois.csv') as file:\n",
    "    db_data = list(csv.DictReader(file))\n",
    "\n",
    "with open('data/raw/labelled_data.csv') as file:\n",
    "    label_data = list(csv.DictReader(file))\n",
    "\n",
    "all_doi = set([i.get('doi') for i in db_data] + [i.get('doi') for i in label_data])\n",
    "doi_set = ar.clean_dois(all_doi)\n",
    "print(f\"There are {len(doi_set.get('clean'))} good DOIs and {len(doi_set.get('removed'))} removed DOIs in the full set of DOIs submitted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doi_set` is a `dict` object. We can write out and review the DOIs that failed the cleaning to see why they failed, or we can continue with our analysis. Here we will simply continue, using the clean results.\n",
    "\n",
    "## Registering Articles\n",
    "\n",
    "The function `ar.register_dois()` accepts a list of DOI values and sbmits them into the database. At the same time, the API itself queries CrossRef to pull in additional metadata. This metadata includes the article title and abstract (when provided by the publisher), along with additional information that may be of use in classification. There is printed output for this function, but here we set `verbose = False` so that we don't generate a massive output here. Depending on the number of DOIs inserted or submitted this process may take some time, in part because it reaches out to the CrossRef API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection failed for DOI {'doi': '10.3760/cma.j.cn112150-20220508-00458'}:\n",
      "HTTPConnectionPool(host='localhost', port=8080): Read timed out. (read timeout=10)\n",
      "Connection failed for DOI {'doi': '10.3760/cma.j.cn112150-20220428-00425'}:\n",
      "HTTPConnectionPool(host='localhost', port=8080): Read timed out. (read timeout=10)\n",
      "Connection failed for DOI {'doi': '10.3760/cma.j.cn115330-20200929-00778'}:\n",
      "HTTPConnectionPool(host='localhost', port=8080): Read timed out. (read timeout=10)\n",
      "Connection failed for DOI {'doi': '10.3760/cma.j.cn115330-20210701-00416'}:\n",
      "HTTPConnectionPool(host='localhost', port=8080): Read timed out. (read timeout=10)\n"
     ]
    }
   ],
   "source": [
    "registered = ar.register_dois(doi_set.get('clean'), verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find that this step can be very time consuming since we are pulling in external data that will be used to build the embeddings and model inputs. As with `ar.clean_dois()`, this function returns a `dict` object with the elements `submitted`, `rejected`, `inserted` and `present`. This allows us to see rejections that result from secondary issues (invalid CrossRef endpoints for example) beyond valid DOI paths. We can also import DOIs from multiple sources, without overwriting data that already exists within the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted: {'doi': '10.1007/s00334-011-0339-6'}\n",
      "Rejected: {'doi': '10.3760/cma.j.cn112150-20220508-00458'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Submitted: {registered.get('submitted')[0] or ''}\\nRejected: {registered.get('rejected')[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can check the `rejected` articles to see if we can understand why they may have been rejected. The CrossRef API allows us to see metadata about a particular article using the DOI, for example, we can query our rejected DOI: [https://api.crossref.org/works/10.3760%2Fcma.j.cn112150-20220508-00458](https://api.crossref.org/works/10.3760%2Fcma.j.cn112150-20220508-00458).\n",
    "\n",
    "With this particular article we see that the DOI simply does not resolve. We can use this to clean our input data if we would like to. We can use the `ar.register_dois()` function to add new or corrected DOIs if we choose to update our original input files, or we can load in new external files and update the database.\n",
    "\n",
    "In all cases, the data is added directly to the database within our Docker container, meaning that, as long as we retain the Docker image, the data persists. Assuming we are prepared to add some new DOIs to the database, we can simply call the following (this time using `verbose = True`, the default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 unique DOIs submitted.\n",
      "4 DOIs valid.\n",
      "doi was present: 10.1007/s13355-012-0130-x\n",
      "doi was present: 10.1063/1.4742131\n",
      "doi was present: 10.1590/s0102-69922012000200010\n",
      "doi was present: 10.1090/S0002-9939-2012-11404-2\n"
     ]
    }
   ],
   "source": [
    "new_dois = ['10.1590/s0102-69922012000200010', '10.1090/S0002-9939-2012-11404-2', '10.1063/1.4742131', '10.1007/s13355-012-0130-x']\n",
    "\n",
    "check = ar.register_dois(new_dois)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the Data\n",
    "\n",
    "The data pipeline goes from registering the DOIs, to developing model-specific embeddings. Because of the way data is stored within the database, we can pre-process and develop embeddings for articles using multiple embedding models. In each case we need to define the form of the text string to be transformed. For this we use the `ar.data_preprocessing()` function. It calls to the database for all article metadata that has yet to be embedded with a partcular model. The `ar.data_preprocessing()` function takes the argument `model_name`, which can be any valid model shared on [HuggingFace](https://huggingface.co/models?pipeline_tag=feature-extraction). We chose the `allenai/specter2` model as the default since it was explicitly trained on a large scientific journal Title-Abstract dataset.\n",
    "\n",
    "Within the database, we store the metadata both as a `jsonb` column (`crossrefmeta`) containing the full set of CrossRef metadata, and also in the columns `doi`, `title`, `subtitle`, `author`, `subject`, `abstract`, `containertitle`, `language` `published` and `publisher`. These fields are all drawn from CrossRef metadata and are not consistently filled. Currently the NeotomaART API returns structured JSON data for each article. It is possible to use other fields by [modifying the API code](https://github.com/NeotomaDB/article_project/blob/main/article_api/v0.1/helpers/dois/dois.js#L198), but we pre-defined these fields and return them as a list of JSON objects:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"doi\":\"10.1126/sciadv.aav3809\",\n",
    "    \"title\":\"Central Europe temperature constrained by speleothem fluid inclusion water isotopes over the past 14,000 years\",\n",
    "    \"subtitle\":null,\n",
    "    \"abstract\":\"<jats:p>Past precipitation water sealed in stalagmites from Switzerland gives insight into temperature changes for the past 14,000 years.</jats:p>\",\n",
    "    \"language\":\"en\",\n",
    "    \"containertitle\":\"Science Advances\"\n",
    "}\n",
    "```\n",
    "\n",
    "It is important to note that data from CrossRef is inconsistent. Of the approximately 2,500 articles originally registered, only ~60% of articles had full abstracts, and approximately 10% of records were missing information about language of origin.\n",
    "\n",
    "The data pre-processing step combines title and abstract into a single string element and imputes language (if missing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = ar.data_preprocessing(model_name = 'allenai/specter2_base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the command returns a list of `dict` objects, each with the keys `doi`, `text` and `language`. This list is then passed to the embeddings step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 53601.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embeddings for 0 objects.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/simon/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/adapters/loading.py:165: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = ar.add_embeddings(processed_data, text_col = 'text', model_name = 'allenai/specter2_base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering a Project\n",
    "\n",
    "We can load papers, and build embeddings without defining a particular project we're working on. The purpose of this project structure is to allow multiple projects to share the same embedded data and sets of papers. This simply reduces overhead for everyone using the system.\n",
    "\n",
    "To register a project we only have to define a project name and a simple description. We also first check to see if it exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "project_exists = ar.project_exists('Neotoma Relevance')\n",
    "ar.register_project('Neotoma Relevance', 'A project to manage models for assessing publication relevance for Neotoma.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is stored in the database, and we can begin to associate labels with the project, allowing us to link \"classification\" lables to the project and to the papers, providing the base data for our models.\n",
    "\n",
    "## Adding Labels\n",
    "\n",
    "We've already seen the file `data/raw/neotoma_dois.csv`. It is a file of DOIs for publications that are a part of Neotoma. They are canonically articles that are of interest to the database (since we've entered them already).  To add labels we are going to take this list of DOIs and assign labels to them (I'm going to say that I was the assigner):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/neotoma_dois.csv') as file:\n",
    "    db_data = list(csv.DictReader(file))\n",
    "first_labels = ar.add_paper_labels(label_data, project = 'Neotoma Relevance', create = True)\n",
    "\n",
    "neotoma_labels = [{'doi': i.get('doi'), 'label': 'In Neotoma', 'person': '0000-0002-2700-4605'} for i in db_data]\n",
    "all_labels = ar.add_paper_labels(neotoma_labels, project = 'Neotoma Relevance', create = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Models\n",
    "\n",
    "With labels added for the project we can now begin to take the data and build models. Using the `get_model_data()` function we call for all project data associated with a particular project and embedding model. This function returns a dict with the DOI, the embedding vector and the label. The `data_model` element is a `dict`, returned from the API for any one DOI:\n",
    "\n",
    "```json\n",
    "{\"doi\": \"10.1126/sciadv.aav3809\", \"embeddings\": [-0.5892478, -1.4083375, ..., -0.8913986, 0.13861942, -0.07215089, -0.120212786, -0.5112147, -1.6048986, -0.18262713, -0.95949847, 0.07596018, 0.03217636, -0.81287014, -0.5136357], \"label\": \"Neotoma\"}\n",
    "```\n",
    "\n",
    "The script below takes all papers that have embedding models for the model `allenai/specter2_base`, both labelled and unlabelled. It removes any unlabelled records, so we can properly build the model (since these are of \"unknown\" suitability). For the classifier we want only two classes, 1 and 0. We had four labelled classes in the dataset: `['Neotoma', 'Not Neotoma', 'In Neotoma' and 'Maybe Neotoma']`. For this classification scheme we will assign `0` to the `Not Neotoma` and assume all the other classes are of interest to Neotoma. We use the `re.search()` function to look for the word `Not` as a way to make this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now need to load in the labelled data and do the train/test split\n",
    "data_model = ar.get_model_data(project = \"Neotoma Relevance\", model = \"allenai/specter2_base\")\n",
    "\n",
    "# Remove unlabelled data (not suitable for building the model)\n",
    "data_model = [i for i in data_model if i.get('label') is not None]\n",
    "\n",
    "# Refine the labelling to two classes, and map the classes to integer values (1 and 0).\n",
    "data_model = [dict(item, **{'target': int(bool(re.search(pattern='Not', string=item['label'])))}) for item in data_model]\n",
    "# Convert the embeddings to a list, and then a data frame with columns named `embedding_xxx` where xxx is the embedding dimension.\n",
    "data_embedding = [i['embeddings'] for i in data_model]\n",
    "data_input = pd.DataFrame(data_embedding, columns = [f'embedding_{str(i)}' for i in range(len(data_model[0]['embeddings']))])\n",
    "data_input = data_input.assign(doi = [i['doi'] for i in data_model])\n",
    "data_input = data_input.assign(target = [i['target'] for i in data_model])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One this code block executed, we have a Pandas DataFrame with a column for the DOI, a column for the `target` class (0 or 1) and then the columns for the embeddings that we will use for classification.  Ultimately, any model can be applied to this data, and the model can be stored within the database as a pickle file. Here we use the `sklearn` package to produce our models, and begin by splitting the data into training and testing data objects, using the `target` column as our target, and stratifying the sampling, since our data is highly unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_input.copy(),\n",
    "                                                    data_input['target'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=data_input['target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data split is managed to ensure that the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifiers = [\n",
    "    (LogisticRegression(max_iter=1000), {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'max_iter': [100, 1000, 10000],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    }),\n",
    "    (DecisionTreeClassifier(class_weight=\"balanced\"), {\n",
    "        'max_depth': range(10, 100, 10)\n",
    "    }),\n",
    "    (KNeighborsClassifier(weights='uniform', algorithm='auto'), {\n",
    "        'n_neighbors': range(5, 100, 10)\n",
    "    }),\n",
    "    (BernoulliNB(binarize=0.0), {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0]\n",
    "    }),\n",
    "    (RandomForestClassifier(), {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30]\n",
    "    })\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our classifiers.\n",
    "\n",
    "We build our models and write our the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up features\n",
      "Beginning training\n",
      "Training logisticregression.\n",
      "Starting fit at 2024-10-15_10-58-34\n",
      "Training decisiontreeclassifier.\n",
      "Starting fit at 2024-10-15_10-58-39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kneighborsclassifier.\n",
      "Starting fit at 2024-10-15_10-58-44\n",
      "Training bernoullinb.\n",
      "Starting fit at 2024-10-15_10-58-45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training randomforestclassifier.\n",
      "Starting fit at 2024-10-15_10-58-46\n",
      "finished process; returning results\n"
     ]
    }
   ],
   "source": [
    "resultsDict = ar.relevancePredictTrain(x_train = X_train, y_train = y_train, classifiers = classifiers)\n",
    "with open('results.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(resultsDict['report'], f, indent=4, sort_keys=True, default=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ar.relevancePredict(data_input, model = 'decisiontreeclassifier_2024-09-22_22-30-35.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/raw/newdois.csv', 'r') as file:\n",
    "    new_dois = file.read().splitlines()\n",
    "\n",
    "clean = ar.clean_dois(new_dois)\n",
    "check = ar.register_dois(clean['clean'], verbose = False)\n",
    "\n",
    "processed_data = ar.data_preprocessing(model_name = 'allenai/specter2_base')\n",
    "\n",
    "embeddings = ar.add_embeddings(processed_data, text_col = 'text', model_name = 'allenai/specter2_base')\n",
    "\n",
    "new_data_model = ar.get_model_data(project = None, model = \"allenai/specter2_base\")\n",
    "\n",
    "data_embedding = [i['embeddings'] for i in new_data_model]\n",
    "data_input = pd.DataFrame(data_embedding, columns = [f'embedding_{str(i)}' for i in range(len(new_data_model[0]['embeddings']))])\n",
    "data_input = data_input.assign(doi = [i['doi'] for i in data_model])\n",
    "\n",
    "results = ar.relevancePredict(data_input, model = 'bernoullinb_2024-10-15_10-58-45.joblib')\n",
    "results.to_csv('/tmp/output.csv')\n",
    "\n",
    "goodpapers = results.loc[results['prediction'] == 1]['doi'].tolist()\n",
    "pubs = [ar.get_publication_metadata(i) for i in goodpapers]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
