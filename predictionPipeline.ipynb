{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Pipelines with `article_relevance`\n",
    "\n",
    "The Article relevance tool was designed for use with the [Neotoma Paleoecology Database](https://www.neotomadb.org) to produce a workflow that allows users to supply an updatable list of DOIs from publications. This list of DOIs is then used to extract metadata from [CrossRef](https://crossref.org) that can be constructed into a list of text embeddings from which we can develop predictive models.  The tooling allows us to generate multiple predictive models, along with the ability to perform a grid-search for optimal hyper-parameter tuning for all models.\n",
    "\n",
    "The workflow provides predictive outputs as to whether an article might be suited for inclusion into a research database, a probability estimate for the prediction, as well as time-stamped predictions and the ability of a user to override the prediction. In this way we can test model evolution, and provide the opportunity for curated stewardship of model predictions.\n",
    "\n",
    "![./assets/overview_image.svg](./assets/overview_image.svg)\n",
    "\n",
    "Our goal is to provide a fully developed research infrastructure that connects labelled publication data from a particular research database, to machine learning models trained with classification models to predict article relevance, the suitability of an \"unknown\" journal article for inclusion within the database.\n",
    "\n",
    "## Using the NeotomaART (Article Relevance Tool) Docker Container\n",
    "\n",
    "The [Docker Container](https://github.com/NeotomaDB/article_project) connects three separate elements:\n",
    "\n",
    "* A [Postgres database](https://github.com/NeotomaDB/article_project/tree/main/article_database) with a [pre-defined data schema](https://github.com/NeotomaDB/article_project/blob/main/article_database/create_database.sql).\n",
    "* A [node/Express API](https://github.com/NeotomaDB/article_project/tree/main/article_api) to interface with the database.\n",
    "* A Python package/framework to interact with the API.\n",
    "\n",
    "This structure allows a research team to set up their own local or cloud-based instance of the ART to support data discovery and ingest for particular research groups. The goal of this project is to allow research teams to submit publications relevant to their research project, along with other publications, all identified with DOIs. Models are then built using a range of parameters, saved, and can then be used to predict whether or not \"unseen\" articles are then relevant for the research group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the script\n",
    "\n",
    "Assuming that the Docker container is running, following the instructions within the [`README`](https://github.com/NeotomaDB/article_project/blob/main/README.md), we can load in the packages and begin to run the code. Note that a `.env` file is used here to help manage any environment variables we might need. At present there is only a single variable in `.env`: `API_HOME=\"localhost:8080\"`. This value is pulled from the [`docker-compose` file in `article_project`](https://github.com/NeotomaDB/article_project/blob/main/docker-compose.yml#L20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import article_relevance as ar\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `article_relevance` package includes multiple requirements for proper use. All requirements are placed in an `requirements.txt` file.\n",
    "\n",
    "In addition, at minimum, we require a file with properly labelled DOI data. This data should contain the following columns:\n",
    "\n",
    "* doi: A properly formatted DOI, with only the shoulder and endpoint. e.g., `10.5467/22343.whatever`\n",
    "* label: A categorical label that can be used to identify whether or not the article is suitable for inclusion into the database.\n",
    "\n",
    "It is also possible to load in unlabelled data as a list of DOI values. Throughout we use DOIs and ORCIDs as unique identifiers to link labels, articles, and predictions.\n",
    "\n",
    "This project contains two data resources in the `data` folder:\n",
    "\n",
    "* `raw/neotoma_crossref.csv`: data directly exported from Neotoma (publication is already included in Neotoma).\n",
    "* `raw/labelled_data.csv`: manually labelled data for model training.\n",
    "* `raw/project_2_labelled_data.csv`: data labelled using the [`SMART` application](https://github.com/RTIInternational/SMART)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These objects represent the file elements we'll be working with for the predictive models.\n",
    "\n",
    "## Loading Raw Data\n",
    "\n",
    "For our purposes we want to identify both the data source and the labelling. For any object we want to know the data source and certainty with which it was labelled. Data from our source should be most trustworthy, labelled data should have some indication of the labeller, unlabelled data should reflect that fact.\n",
    "\n",
    "The first set of data we load is raw data from the database itself (`db_data`), and a set of labelled data (`labelled_data.csv`). The labelled data was all labelled by the same person in this case (\"Simon Goring\"), so when we insert the data to the `LABELLING_STORE` document we'll make sure to add that.\n",
    "\n",
    "DOIs get entered in many ways by users. Incorrect data entry may result in DOIs that do not properly resolve, or are simply incorrect. Additionaly data entry errors may result in leading or trailing whitespace. To help us ensure that data is entered correctly we will use the function `ar.clean_dois()`. This function returns a `dict` with the `clean` and `removed` DOIs submitted by the user. This helps support data cleaning down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2652 good DOIs and 48 removed DOIs in the full set of DOIs submitted.\n"
     ]
    }
   ],
   "source": [
    "with open('data/raw/neotoma_dois.csv') as file:\n",
    "    db_data = list(csv.DictReader(file))\n",
    "\n",
    "with open('data/raw/labelled_data.csv') as file:\n",
    "    label_data = list(csv.DictReader(file))\n",
    "\n",
    "all_doi = set([i.get('doi') for i in db_data] + [i.get('doi') for i in label_data])\n",
    "doi_set = ar.clean_dois(all_doi)\n",
    "print(f\"There are {len(doi_set.get('clean'))} good DOIs and {len(doi_set.get('removed'))} removed DOIs in the full set of DOIs submitted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doi_set` is a `dict` object. We can write out and review the DOIs that failed the cleaning to see why they failed, or we can continue with our analysis. Here we will simply continue, using the clean results.\n",
    "\n",
    "## Registering Articles\n",
    "\n",
    "The function `ar.register_dois()` accepts a list of DOI values and sbmits them into the database. At the same time, the API itself queries CrossRef to pull in additional metadata. This metadata includes the article title and abstract (when provided by the publisher), along with additional information that may be of use in classification. There is printed output for this function, but here we set `verbose = False` so that we don't generate a massive output here. Depending on the number of DOIs inserted or submitted this process may take some time, in part because it reaches out to the CrossRef API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection failed for DOI {'doi': '10.3760/cma.j.cn112150-20220508-00458'}:\n",
      "HTTPConnectionPool(host='localhost', port=8080): Read timed out. (read timeout=10)\n",
      "Connection failed for DOI {'doi': '10.3760/cma.j.cn112150-20220428-00425'}:\n",
      "HTTPConnectionPool(host='localhost', port=8080): Read timed out. (read timeout=10)\n",
      "Connection failed for DOI {'doi': '10.3760/cma.j.cn115330-20200929-00778'}:\n",
      "HTTPConnectionPool(host='localhost', port=8080): Read timed out. (read timeout=10)\n",
      "Connection failed for DOI {'doi': '10.3760/cma.j.cn115330-20210701-00416'}:\n",
      "HTTPConnectionPool(host='localhost', port=8080): Read timed out. (read timeout=10)\n"
     ]
    }
   ],
   "source": [
    "registered = ar.register_dois(doi_set.get('clean'), verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find that this step can be very time consuming since we are pulling in external data that will be used to build the embeddings and model inputs. As with `ar.clean_dois()`, this function returns a `dict` object with the elements `submitted`, `rejected`, `inserted` and `present`. This allows us to see rejections that result from secondary issues (invalid CrossRef endpoints for example) beyond valid DOI paths. We can also import DOIs from multiple sources, without overwriting data that already exists within the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted: {'doi': '10.1007/s00334-011-0339-6'}\n",
      "Rejected: {'doi': '10.3760/cma.j.cn112150-20220508-00458'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Submitted: {registered.get('submitted')[0] or ''}\\nRejected: {registered.get('rejected')[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can check the `rejected` articles to see if we can understand why they may have been rejected. The CrossRef API allows us to see metadata about a particular article using the DOI, for example, we can query our rejected DOI: [https://api.crossref.org/works/10.3760%2Fcma.j.cn112150-20220508-00458](https://api.crossref.org/works/10.3760%2Fcma.j.cn112150-20220508-00458).\n",
    "\n",
    "With this particular article we see that the DOI simply does not resolve. We can use this to clean our input data if we would like to. We can use the `ar.register_dois()` function to add new or corrected DOIs if we choose to update our original input files, or we can load in new external files and update the database.\n",
    "\n",
    "In all cases, the data is added directly to the database within our Docker container, meaning that, as long as we retain the Docker image, the data persists. Assuming we are prepared to add some new DOIs to the database, we can simply call the following (this time using `verbose = True`, the default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 unique DOIs submitted.\n",
      "4 DOIs valid.\n",
      "doi was present: 10.1007/s13355-012-0130-x\n",
      "doi was present: 10.1063/1.4742131\n",
      "doi was present: 10.1590/s0102-69922012000200010\n",
      "doi was present: 10.1090/S0002-9939-2012-11404-2\n"
     ]
    }
   ],
   "source": [
    "new_dois = ['10.1590/s0102-69922012000200010', '10.1090/S0002-9939-2012-11404-2', '10.1063/1.4742131', '10.1007/s13355-012-0130-x']\n",
    "\n",
    "check = ar.register_dois(new_dois)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the Data\n",
    "\n",
    "The data pipeline goes from registering the DOIs, to developing model-specific embeddings. Because of the way data is stored within the database, we can pre-process and develop embeddings for articles using multiple embedding models. In each case we need to define the form of the text string to be transformed. For this we use the `ar.data_preprocessing()` function. It calls to the database for all article metadata that has yet to be embedded with a partcular model. The `ar.data_preprocessing()` function takes the argument `model_name`, which can be any valid model shared on [HuggingFace](https://huggingface.co/models?pipeline_tag=feature-extraction). We chose the `allenai/specter2` model as the default since it was explicitly trained on a large scientific journal Title-Abstract dataset.\n",
    "\n",
    "Within the database, we store the metadata both as a `jsonb` column (`crossrefmeta`) containing the full set of CrossRef metadata, and also in the columns `doi`, `title`, `subtitle`, `author`, `subject`, `abstract`, `containertitle`, `language` `published` and `publisher`. These fields are all drawn from CrossRef metadata and are not consistently filled. Currently the NeotomaART API returns structured JSON data for each article. It is possible to use other fields by [modifying the API code](https://github.com/NeotomaDB/article_project/blob/main/article_api/v0.1/helpers/dois/dois.js#L198), but we pre-defined these fields and return them as a list of JSON objects:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"doi\":\"10.1126/sciadv.aav3809\",\n",
    "    \"title\":\"Central Europe temperature constrained by speleothem fluid inclusion water isotopes over the past 14,000 years\",\n",
    "    \"subtitle\":null,\n",
    "    \"abstract\":\"<jats:p>Past precipitation water sealed in stalagmites from Switzerland gives insight into temperature changes for the past 14,000 years.</jats:p>\",\n",
    "    \"language\":\"en\",\n",
    "    \"containertitle\":\"Science Advances\"\n",
    "}\n",
    "```\n",
    "\n",
    "It is important to note that data from CrossRef is inconsistent. Of the approximately 2,500 articles originally registered, only ~60% of articles had full abstracts, and approximately 10% of records were missing information about language of origin.\n",
    "\n",
    "The data pre-processing step combines title and abstract into a single string element and imputes language (if missing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = ar.data_preprocessing(model_name = 'allenai/specter2_base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the command returns a list of `dict` objects, each with the keys `doi`, `text` and `language`. This list is then passed to the embeddings step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 53601.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embeddings for 0 objects.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/simon/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/adapters/loading.py:165: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = ar.add_embeddings(processed_data, text_col = 'text', model_name = 'allenai/specter2_base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering a Project\n",
    "\n",
    "We can load papers, and build embeddings without defining a particular project we're working on. The purpose of this project structure is to allow multiple projects to share the same embedded data and sets of papers. This simply reduces overhead for everyone using the system.\n",
    "\n",
    "To register a project we only have to define a project name and a simple description. We also first check to see if it exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "project_exists = ar.project_exists('Neotoma Relevance')\n",
    "ar.register_project('Neotoma Relevance', 'A project to manage models for assessing publication relevance for Neotoma.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is stored in the database, and we can begin to associate labels with the project, allowing us to link \"classification\" lables to the project and to the papers, providing the base data for our models.\n",
    "\n",
    "## Adding Labels\n",
    "\n",
    "We've already seen the file `data/raw/neotoma_dois.csv`. It is a file of DOIs for publications that are a part of Neotoma. They are canonically articles that are of interest to the database (since we've entered them already).  To add labels we are going to take this list of DOIs and assign labels to them (I'm going to say that I was the assigner):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/neotoma_dois.csv') as file:\n",
    "    db_data = list(csv.DictReader(file))\n",
    "first_labels = ar.add_paper_labels(label_data, project = 'Neotoma Relevance', create = True)\n",
    "\n",
    "neotoma_labels = [{'doi': i.get('doi'), 'label': 'In Neotoma', 'person': '0000-0002-2700-4605'} for i in db_data]\n",
    "all_labels = ar.add_paper_labels(neotoma_labels, project = 'Neotoma Relevance', create = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Models\n",
    "\n",
    "With labels added for the project we can now begin to take the data and build models. Using the `get_model_data()` function we call for all project data associated with a particular project and embedding model. This function returns a dict with the DOI, the embedding vector and the label. The `data_model` element is a `dict`, returned from the API for any one DOI:\n",
    "\n",
    "```json\n",
    "{\"doi\": \"10.1126/sciadv.aav3809\", \"embeddings\": [-0.5892478, -1.4083375, ..., -0.8913986, 0.13861942, -0.07215089, -0.120212786, -0.5112147, -1.6048986, -0.18262713, -0.95949847, 0.07596018, 0.03217636, -0.81287014, -0.5136357], \"label\": \"Neotoma\"}\n",
    "```\n",
    "\n",
    "The script below takes all papers that have embedding models for the model `allenai/specter2_base`, both labelled and unlabelled. It removes any unlabelled records, so we can properly build the model (since these are of \"unknown\" suitability). For the classifier we want only two classes, 1 and 0. We had four labelled classes in the dataset: `['Neotoma', 'Not Neotoma', 'In Neotoma' and 'Maybe Neotoma']`. For this classification scheme we will assign `0` to the `Not Neotoma` and assume all the other classes are of interest to Neotoma. We use the `re.search()` function to look for the word `Not` as a way to make this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now need to load in the labelled data and do the train/test split\n",
    "data_model = ar.get_model_data(project = \"Neotoma Relevance\", model = \"allenai/specter2_base\")\n",
    "\n",
    "# Remove unlabelled data (not suitable for building the model)\n",
    "data_model = [i for i in data_model if i.get('label') is not None]\n",
    "\n",
    "# Refine the labelling to two classes, and map the classes to integer values (1 and 0).\n",
    "data_model = [dict(item, **{'target': int(bool(re.search(pattern='Not', string=item['label'])))}) for item in data_model]\n",
    "# Convert the embeddings to a list, and then a data frame with columns named `embedding_xxx` where xxx is the embedding dimension.\n",
    "data_embedding = [i['embeddings'] for i in data_model]\n",
    "data_input = pd.DataFrame(data_embedding, columns = [f'embedding_{str(i)}' for i in range(len(data_model[0]['embeddings']))])\n",
    "data_input = data_input.assign(doi = [i['doi'] for i in data_model])\n",
    "data_input = data_input.assign(target = [i['target'] for i in data_model])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this code block has executed, we have a Pandas DataFrame with a column for the DOI, a column for the `target` class (0 or 1) and then the columns for the embeddings that we will use for classification.  Ultimately, any model supported by `scikitlearn` can be applied to the text embeddings, and the model can be stored within the database as a `joblib` file (note that `joblib` files have particular security vulnerabilities, so avoid using or opening `joblib` files from outside trusted environments).\n",
    "\n",
    "Here we use the `sklearn` package to produce our models, and begin by splitting the data into training and testing data objects, using the `target` column as our target, and stratifying the sampling, since our data is highly unbalanced. We have set a specific `random_state` here for reproducibility purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_input.copy(),\n",
    "                                                    data_input['target'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=data_input['target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "\n",
    "As mentioned, it is possible to use multiple classifiers, defined through imports from `sklearn`. The `ar.relevancePredictTrain()` function uses a randomized search across parameters by default. This means that we can define parameter ranges for any of the classifier parameters (as defined in their relevant help documentation). We use these models as examples to showcase support for multi-model approaches. It is possible to further tune models, providing \"fixed\" model parameterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifiers = [\n",
    "    (LogisticRegression(max_iter=1000), {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'max_iter': [100, 1000, 10000],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    }),\n",
    "    (DecisionTreeClassifier(class_weight=\"balanced\"), {\n",
    "        'max_depth': range(10, 100, 10)\n",
    "    }),\n",
    "    (KNeighborsClassifier(weights='uniform', algorithm='auto'), {\n",
    "        'n_neighbors': range(5, 100, 10)\n",
    "    }),\n",
    "    (BernoulliNB(binarize=0.0), {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0]\n",
    "    }),\n",
    "    (RandomForestClassifier(), {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30]\n",
    "    })\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our classifiers using lists or range functions, passed in for each parameter of interest. Once the classifiers have been defined we can then run the model using `ar.relevancePredictTrain()`. This function directly outputs a dictionary of model results (accuracy, recall, precision and the F1 statistic for both test and training sets). In addition to these model assessment values, the function also outputs the optimized model for each of the classification types as a `joblib` file in the `data/models` directory. The files are named using the model name and a timestamp string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up features\n",
      "Beginning training\n",
      "Training logisticregression.\n",
      "Starting fit at 2024-10-15_10-58-34\n",
      "Training decisiontreeclassifier.\n",
      "Starting fit at 2024-10-15_10-58-39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kneighborsclassifier.\n",
      "Starting fit at 2024-10-15_10-58-44\n",
      "Training bernoullinb.\n",
      "Starting fit at 2024-10-15_10-58-45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training randomforestclassifier.\n",
      "Starting fit at 2024-10-15_10-58-46\n",
      "finished process; returning results\n"
     ]
    }
   ],
   "source": [
    "resultsDict = ar.relevancePredictTrain(x_train = X_train, y_train = y_train, classifiers = classifiers)\n",
    "with open('results.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(resultsDict['report'], f, indent=4, sort_keys=True, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': ['LogisticRegression',\n",
       "  'DecisionTreeClassifier',\n",
       "  'KNeighborsClassifier',\n",
       "  'BernoulliNB',\n",
       "  'RandomForestClassifier'],\n",
       " 'model': [Pipeline(steps=[('columntransformer',\n",
       "                   ColumnTransformer(remainder='passthrough',\n",
       "                                     transformers=[('doi', 'drop', ['doi'])])),\n",
       "                  ('simpleimputer',\n",
       "                   SimpleImputer(fill_value=0, strategy='constant')),\n",
       "                  ('logisticregression',\n",
       "                   LogisticRegression(C=0.01, max_iter=1000,\n",
       "                                      solver='liblinear'))]),\n",
       "  Pipeline(steps=[('columntransformer',\n",
       "                   ColumnTransformer(remainder='passthrough',\n",
       "                                     transformers=[('doi', 'drop', ['doi'])])),\n",
       "                  ('simpleimputer',\n",
       "                   SimpleImputer(fill_value=0, strategy='constant')),\n",
       "                  ('decisiontreeclassifier',\n",
       "                   DecisionTreeClassifier(class_weight='balanced',\n",
       "                                          max_depth=60))]),\n",
       "  Pipeline(steps=[('columntransformer',\n",
       "                   ColumnTransformer(remainder='passthrough',\n",
       "                                     transformers=[('doi', 'drop', ['doi'])])),\n",
       "                  ('simpleimputer',\n",
       "                   SimpleImputer(fill_value=0, strategy='constant')),\n",
       "                  ('kneighborsclassifier', KNeighborsClassifier(n_neighbors=15))]),\n",
       "  Pipeline(steps=[('columntransformer',\n",
       "                   ColumnTransformer(remainder='passthrough',\n",
       "                                     transformers=[('doi', 'drop', ['doi'])])),\n",
       "                  ('simpleimputer',\n",
       "                   SimpleImputer(fill_value=0, strategy='constant')),\n",
       "                  ('bernoullinb', BernoulliNB(alpha=0.001))]),\n",
       "  Pipeline(steps=[('columntransformer',\n",
       "                   ColumnTransformer(remainder='passthrough',\n",
       "                                     transformers=[('doi', 'drop', ['doi'])])),\n",
       "                  ('simpleimputer',\n",
       "                   SimpleImputer(fill_value=0, strategy='constant')),\n",
       "                  ('randomforestclassifier',\n",
       "                   RandomForestClassifier(max_depth=20, n_estimators=200))])],\n",
       " 'report': [{'classifier': ['LogisticRegression',\n",
       "    'DecisionTreeClassifier',\n",
       "    'KNeighborsClassifier',\n",
       "    'BernoulliNB',\n",
       "    'RandomForestClassifier'],\n",
       "   'Fit Time': [datetime.timedelta(seconds=4, microseconds=949283),\n",
       "    datetime.timedelta(seconds=5, microseconds=435514),\n",
       "    datetime.timedelta(seconds=1, microseconds=324917),\n",
       "    datetime.timedelta(microseconds=411270),\n",
       "    datetime.timedelta(seconds=12, microseconds=970719)],\n",
       "   'train_recall': [np.float64(0.9661627673558723),\n",
       "    np.float64(0.9994599459945995),\n",
       "    np.float64(0.9449251759708345),\n",
       "    np.float64(0.8342338550401802),\n",
       "    np.float64(1.0)],\n",
       "   'train_f1': [np.float64(0.9583158172884806),\n",
       "    np.float64(0.9997298514182802),\n",
       "    np.float64(0.9499658574452777),\n",
       "    np.float64(0.8966897667206732),\n",
       "    np.float64(0.9997300944669366)],\n",
       "   'train_precision': [np.float64(0.9505986042811163),\n",
       "    np.float64(1.0),\n",
       "    np.float64(0.9550747592961126),\n",
       "    np.float64(0.9692593043822175),\n",
       "    np.float64(0.9994604316546762)],\n",
       "   'train_accuracy': [np.float64(0.9414786967418547),\n",
       "    np.float64(0.9996240601503759),\n",
       "    np.float64(0.9307017543859649),\n",
       "    np.float64(0.8661654135338346),\n",
       "    np.float64(0.9996240601503759)],\n",
       "   'test_recall': [np.float64(0.956081344310833),\n",
       "    np.float64(0.92152559540815),\n",
       "    np.float64(0.935927070617874),\n",
       "    np.float64(0.833690881230034),\n",
       "    np.float64(0.9640002077760175)],\n",
       "   'test_f1': [np.float64(0.9489055690157573),\n",
       "    np.float64(0.9136490075897307),\n",
       "    np.float64(0.9434212965318108),\n",
       "    np.float64(0.8945622252946108),\n",
       "    np.float64(0.9480360470167802)],\n",
       "   'test_precision': [np.float64(0.9421442521256859),\n",
       "    np.float64(0.906094302877148),\n",
       "    np.float64(0.9511201980081821),\n",
       "    np.float64(0.9651176388667672),\n",
       "    np.float64(0.9328867073242797)],\n",
       "   'test_accuracy': [np.float64(0.9283208020050125),\n",
       "    np.float64(0.8786967418546366),\n",
       "    np.float64(0.9218045112781954),\n",
       "    np.float64(0.8631578947368421),\n",
       "    np.float64(0.9263157894736842)]}],\n",
       " 'date': [datetime.datetime(2024, 10, 15, 10, 58, 59, 211821)]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `resultsDict` object also includes information about the \"best\" fit model.\n",
    "\n",
    "## Prediction\n",
    "\n",
    "Given any of the models of interest, we can then call the `ar.relevancePredict()` function, which will load the `joblib` file and use the model to predict values based on the embeddings provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ar.relevancePredict(data_input, model = 'decisiontreeclassifier_2024-09-22_22-30-35.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Full Run-Through:\n",
    "\n",
    "Here we see a complete run through using a pre-existing model and \"new\" DOIs that have not been examined before. The output is a set of publication metadata that is relevant to the Database, based on our prior labelled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     new_dois \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m      4\u001b[0m clean \u001b[38;5;241m=\u001b[39m ar\u001b[38;5;241m.\u001b[39mclean_dois(new_dois)\n\u001b[0;32m----> 5\u001b[0m check \u001b[38;5;241m=\u001b[39m \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_dois\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m ar\u001b[38;5;241m.\u001b[39mdata_preprocessing(model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallenai/specter2_base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m ar\u001b[38;5;241m.\u001b[39madd_embeddings(processed_data, text_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallenai/specter2_base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Neotoma/article-relevance/article_relevance/register_apis.py:145\u001b[0m, in \u001b[0;36mregister_dois\u001b[0;34m(dois, verbose)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m valid_doi\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cleaned_entries\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(cleaned_entries\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremoved\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unique DOIs submitted.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_doi)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m DOIs valid.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Documents/Neotoma/article-relevance/.venv/lib/python3.12/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('./data/raw/newdois.csv', 'r') as file:\n",
    "    new_dois = file.read().splitlines()\n",
    "\n",
    "# Clean and register DOIs\n",
    "clean = ar.clean_dois(new_dois)\n",
    "check = ar.register_dois(clean['clean'], verbose = False)\n",
    "\n",
    "# Get text objects for papers that haven't been processed by the named model:\n",
    "processed_data = ar.data_preprocessing(model_name = 'allenai/specter2_base')\n",
    "\n",
    "# Build embeddings locally.\n",
    "embeddings = ar.add_embeddings(processed_data, text_col = 'text', model_name = 'allenai/specter2_base')\n",
    "\n",
    "new_data_model = ar.get_model_data(project = None, model = \"allenai/specter2_base\")\n",
    "\n",
    "data_embedding = [i['embeddings'] for i in new_data_model]\n",
    "data_input = pd.DataFrame(data_embedding, columns = [f'embedding_{str(i)}' for i in range(len(new_data_model[0]['embeddings']))])\n",
    "data_input = data_input.assign(doi = [i['doi'] for i in data_model])\n",
    "\n",
    "results = ar.relevancePredict(data_input, model = 'bernoullinb_2024-10-15_10-58-45.joblib')\n",
    "results.to_csv('/tmp/output.csv')\n",
    "\n",
    "goodpapers = results.loc[results['prediction'] == 1]['doi'].tolist()\n",
    "pubs = [ar.get_publication_metadata(i) for i in goodpapers]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
